{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVlNqBsKGbRmKW6EKWURC9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2qlc9lbCXake","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"f68c3e64-f9d1-4323-fa2a-c0a29bc72906","executionInfo":{"status":"ok","timestamp":1582886244335,"user_tz":480,"elapsed":94431,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 134443 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.17-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ho9iO_a-XezO","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive  -o nonempty"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfjzEzjbXoyN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"808db4f4-f2ca-48e4-8162-d3ced61e68db","executionInfo":{"status":"ok","timestamp":1582886269995,"user_tz":480,"elapsed":18101,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}}},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/SeqGAN-master\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JfMQM9wXXsAA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"07c53276-084e-4789-b03a-480721a9c198"},"source":["\n","os.chdir(path)\n","os.listdir(path)\n","\n","import numpy as np\n","import tensorflow as tf\n","import random\n","from dataloader import Gen_Data_loader, Dis_dataloader\n","from generator import Generator\n","from discriminator import Discriminator\n","from rollout import ROLLOUT\n","from target_lstm import TARGET_LSTM\n","import pickle\n","\n","#########################################################################################\n","#  Generator  Hyper-parameters\n","######################################################################################\n","EMB_DIM = 32 # embedding dimension\n","HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n","SEQ_LENGTH = 20 # sequence length\n","START_TOKEN = 0\n","PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n","SEED = 88\n","BATCH_SIZE = 64\n","\n","#########################################################################################\n","#  Discriminator  Hyper-parameters\n","#########################################################################################\n","dis_embedding_dim = 64\n","dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n","dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n","dis_dropout_keep_prob = 0.75\n","dis_l2_reg_lambda = 0.2\n","dis_batch_size = 64\n","\n","#########################################################################################\n","#  Basic Training Parameters\n","#########################################################################################\n","TOTAL_BATCH = 200\n","positive_file = 'save/data_preprocessing.txt'\n","negative_file = 'save/generator_sample.txt'\n","eval_file = 'save/eval_file.txt'\n","generated_num = 10000\n","\n","\n","def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n","    # Generate Samples\n","    generated_samples = []\n","    for _ in range(int(generated_num / batch_size)):\n","        generated_samples.extend(trainable_model.generate(sess))\n","\n","    with open(output_file, 'w') as fout:\n","        for poem in generated_samples:\n","            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n","            fout.write(buffer)\n","\n","\n","def target_loss(sess, target_lstm, data_loader):\n","    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n","    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n","    nll = []\n","    data_loader.reset_pointer()\n","\n","    for it in range(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n","        nll.append(g_loss)\n","\n","    return np.mean(nll)\n","\n","\n","def pre_train_epoch(sess, trainable_model, data_loader):\n","    # Pre-train the generator using MLE for one epoch\n","    supervised_g_losses = []\n","    data_loader.reset_pointer()\n","\n","    for it in range(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        _, g_loss = trainable_model.pretrain_step(sess, batch)\n","        supervised_g_losses.append(g_loss)\n","\n","    return np.mean(supervised_g_losses)\n","\n","\n","def main():\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    assert START_TOKEN == 0\n","\n","    gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n","    likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n","    vocab_size = 5000\n","    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n","\n","    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n","    target_params = pickle.load(open('save/target_params_py3.pkl','rb'))\n","    target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n","\n","    discriminator = Discriminator(sequence_length=20, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n","                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    sess.run(tf.global_variables_initializer())\n","\n","    # First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n","    #generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n","    gen_data_loader.create_batches(positive_file)\n","\n","    log = open('save/experiment-log.txt', 'w')\n","    #  pre-train generator\n","    print ('Start pre-training...')\n","    log.write('pre-training...\\n')\n","    for epoch in range(PRE_EPOCH_NUM):\n","        loss = pre_train_epoch(sess, generator, gen_data_loader)\n","        if epoch % 5 == 0:\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","            likelihood_data_loader.create_batches(eval_file)\n","            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n","            print ('pre-train epoch ', epoch, 'test_loss ', test_loss)\n","            buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n","            log.write(buffer)\n","\n","    print ('Start pre-training discriminator...')\n","    # Train 3 epoch on the generated data and do this for 50 times\n","    for _ in range(50):\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","        dis_data_loader.load_train_data(positive_file, negative_file)\n","        for _ in range(3):\n","            dis_data_loader.reset_pointer()\n","            for it in range(dis_data_loader.num_batch):\n","                x_batch, y_batch = dis_data_loader.next_batch()\n","                feed = {\n","                    discriminator.input_x: x_batch,\n","                    discriminator.input_y: y_batch,\n","                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                }\n","                _ = sess.run(discriminator.train_op, feed)\n","\n","    rollout = ROLLOUT(generator, 0.8)\n","\n","    print ('#########################################################################')\n","    print ('Start Adversarial Training...')\n","    log.write('adversarial training...\\n')\n","    for total_batch in range(TOTAL_BATCH):\n","        # Train the generator for one step\n","        for it in range(1):\n","            samples = generator.generate(sess)\n","            rewards = rollout.get_reward(sess, samples, 16, discriminator)\n","            feed = {generator.x: samples, generator.rewards: rewards}\n","            _ = sess.run(generator.g_updates, feed_dict=feed)\n","\n","        # Test\n","        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","            likelihood_data_loader.create_batches(eval_file)\n","            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n","            buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n","            print ('total_batch: ', total_batch, 'test_loss: ', test_loss)\n","            log.write(buffer)\n","\n","        # Update roll-out parameters\n","        rollout.update_params()\n","\n","        # Train the discriminator\n","        for _ in range(5):\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","            dis_data_loader.load_train_data(positive_file, negative_file)\n","\n","            for _ in range(3):\n","                dis_data_loader.reset_pointer()\n","                for it in range(dis_data_loader.num_batch):\n","                    x_batch, y_batch = dis_data_loader.next_batch()\n","                    feed = {\n","                        discriminator.input_x: x_batch,\n","                        discriminator.input_y: y_batch,\n","                        discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                    }\n","                    _ = sess.run(discriminator.train_op, feed)\n","\n","    log.close()\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:24: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:129: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:50: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:51: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.random.categorical` instead.\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:209: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/target_lstm.py:17: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:74: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:85: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:96: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:28: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:29: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:123: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:129: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:132: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","Start pre-training...\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,0] = 111111423 is not in [0, 5000)\n\t [[{{node embedding_lookup}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-92653469a9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-92653469a9fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pre-training...\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRE_EPOCH_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-92653469a9fe>\u001b[0m in \u001b[0;36mpre_train_epoch\u001b[0;34m(sess, trainable_model, data_loader)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0msupervised_g_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/SeqGAN-master/generator.py\u001b[0m in \u001b[0;36mpretrain_step\u001b[0;34m(self, sess, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,0] = 111111423 is not in [0, 5000)\n\t [[node embedding_lookup (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'embedding_lookup':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-92653469a9fe>\", line 186, in <module>\n    main()\n  File \"<ipython-input-4-92653469a9fe>\", line 95, in main\n    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n  File \"/content/drive/My Drive/SeqGAN-master/generator.py\", line 36, in __init__\n    self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/embedding_ops.py\", line 317, in embedding_lookup\n    transform_fn=None)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/embedding_ops.py\", line 135, in _embedding_lookup_and_transform\n    array_ops.gather(params[0], ids, name=name), ids, max_norm)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 3956, in gather\n    params, indices, axis, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4082, in gather_v2\n    batch_dims=batch_dims, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"]}]}]}