{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"l1816EZ9XkCN","colab_type":"code","outputId":"6f1d4888-dac1-4f43-e92e-2cced90c5782","executionInfo":{"status":"ok","timestamp":1582974677572,"user_tz":480,"elapsed":48675,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 145113 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.17-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zkP-BUW8XoqQ","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive  -o nonempty"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4-mLh3sXp-J","colab_type":"code","outputId":"a0bb1f86-cd7a-4d70-e250-a0536841a75d","executionInfo":{"status":"ok","timestamp":1582974698261,"user_tz":480,"elapsed":16105,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/SeqGAN-master\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OLLt05P7Xrf_","colab_type":"code","outputId":"b7b23105-506b-47bb-c284-5daa1f28cff1","executionInfo":{"status":"ok","timestamp":1582985966912,"user_tz":480,"elapsed":11264136,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","os.chdir(path)\n","os.listdir(path)\n","\n","import numpy as np\n","import tensorflow as tf\n","import random\n","from dataloader import Gen_Data_loader, Dis_dataloader\n","from generator import Generator\n","from discriminator import Discriminator\n","from rollout import ROLLOUT\n","from target_lstm import TARGET_LSTM\n","import pickle\n","\n","#########################################################################################\n","#  Generator  Hyper-parameters\n","######################################################################################\n","EMB_DIM = 32 # embedding dimension\n","HIDDEN_DIM = 100 # hidden state dimension of lstm cell\n","SEQ_LENGTH = 20 # sequence length\n","START_TOKEN = 0\n","PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n","SEED = 88\n","BATCH_SIZE = 64\n","\n","#########################################################################################\n","#  Discriminator  Hyper-parameters\n","#########################################################################################\n","dis_embedding_dim = 64\n","dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15]\n","dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160]\n","dis_dropout_keep_prob = 0.25\n","dis_l2_reg_lambda = 0.2\n","dis_batch_size = 64\n","\n","#########################################################################################\n","#  Basic Training Parameters\n","#########################################################################################\n","TOTAL_BATCH = 200\n","positive_file = 'save/data_preprocessing_3.txt'\n","negative_file = 'save/generator_sample_tem.txt'\n","eval_file = 'save/eval_file.txt'\n","generated_num = 10000\n","\n","\n","def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n","    # Generate Samples\n","    generated_samples = []\n","    for _ in range(int(generated_num / batch_size)):\n","        generated_samples.extend(trainable_model.generate(sess))\n","\n","    with open(output_file, 'w') as fout:\n","        for poem in generated_samples:\n","            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n","            fout.write(buffer)\n","\n","\n","def target_loss(sess, target_lstm, data_loader):\n","    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n","    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n","    nll = []\n","    data_loader.reset_pointer()\n","\n","    for it in range(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n","        nll.append(g_loss)\n","\n","    return np.mean(nll)\n","\n","\n","def pre_train_epoch(sess, trainable_model, data_loader):\n","    # Pre-train the generator using MLE for one epoch\n","    supervised_g_losses = []\n","    data_loader.reset_pointer()\n","\n","    for it in range(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        _, g_loss = trainable_model.pretrain_step(sess, batch)\n","        supervised_g_losses.append(g_loss)\n","\n","    return np.mean(supervised_g_losses)\n","\n","\n","def main():\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    assert START_TOKEN == 0\n","\n","    gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n","    likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n","    vocab_size = 19200\n","    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n","\n","    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n","    #target_params = pickle.load(open('save/target_params_py3.pkl','rb'))#这个文件什么用？？？？\n","    #target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n","\n","    discriminator = Discriminator(sequence_length=20, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n","                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    sess.run(tf.global_variables_initializer())\n","\n","    # First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n","    #generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n","    gen_data_loader.create_batches(positive_file)\n","\n","    log = open('save/experiment-log.txt', 'w')\n","    #  pre-train generator\n","    print ('Start pre-training...')\n","    log.write('pre-training...\\n')\n","    for epoch in range(PRE_EPOCH_NUM):\n","        loss = pre_train_epoch(sess, generator, gen_data_loader)\n","        if epoch % 5 == 0:\n","#            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","#            likelihood_data_loader.create_batches(eval_file)\n","#            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n","            print ('pre-train epoch ', epoch)\n","#            buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n","#            log.write(buffer)\n","\n","    print ('Start pre-training discriminator...')\n","    # Train 3 epoch on the generated data and do this for 50 times\n","    for _ in range(50):\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","        dis_data_loader.load_train_data(positive_file, negative_file)\n","        for _ in range(3):\n","            dis_data_loader.reset_pointer()\n","            for it in range(dis_data_loader.num_batch):\n","                x_batch, y_batch = dis_data_loader.next_batch()\n","                feed = {\n","                    discriminator.input_x: x_batch,\n","                    discriminator.input_y: y_batch,\n","                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                }\n","                _ = sess.run(discriminator.train_op, feed)\n","\n","    rollout = ROLLOUT(generator, 0.8)\n","\n","    print ('#########################################################################')\n","    print ('Start Adversarial Training...')\n","    log.write('adversarial training...\\n')\n","    for total_batch in range(TOTAL_BATCH):\n","        # Train the generator for one step\n","        for it in range(1):\n","            samples = generator.generate(sess)\n","            rewards = rollout.get_reward(sess, samples, 16, discriminator)\n","            feed = {generator.x: samples, generator.rewards: rewards}\n","            _ = sess.run(generator.g_updates, feed_dict=feed)\n","\n","        # Test\n","        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n","#            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","#            likelihood_data_loader.create_batches(eval_file)\n","#            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n","#            buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n","            print ('total_batch: ', total_batch)#, 'test_loss: ', test_loss)\n","#            log.write(buffer)\n","\n","        # Update roll-out parameters\n","        rollout.update_params()\n","\n","        # Train the discriminator\n","        for _ in range(5):\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","            dis_data_loader.load_train_data(positive_file, negative_file)\n","\n","            for _ in range(3):\n","                dis_data_loader.reset_pointer()\n","                for it in range(dis_data_loader.num_batch):\n","                    x_batch, y_batch = dis_data_loader.next_batch()\n","                    feed = {\n","                        discriminator.input_x: x_batch,\n","                        discriminator.input_y: y_batch,\n","                        discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                    }\n","                    _ = sess.run(discriminator.train_op, feed)\n","\n","    log.close()\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:24: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:129: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:50: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:51: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.random.categorical` instead.\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/generator.py:209: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:74: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:85: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:96: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:28: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:29: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:123: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:129: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/SeqGAN-master/discriminator.py:132: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","Start pre-training...\n","pre-train epoch  0\n","pre-train epoch  5\n","pre-train epoch  10\n","pre-train epoch  15\n","pre-train epoch  20\n","pre-train epoch  25\n","pre-train epoch  30\n","pre-train epoch  35\n","pre-train epoch  40\n","pre-train epoch  45\n","pre-train epoch  50\n","pre-train epoch  55\n","pre-train epoch  60\n","pre-train epoch  65\n","pre-train epoch  70\n","pre-train epoch  75\n","pre-train epoch  80\n","pre-train epoch  85\n","pre-train epoch  90\n","pre-train epoch  95\n","pre-train epoch  100\n","pre-train epoch  105\n","pre-train epoch  110\n","pre-train epoch  115\n","Start pre-training discriminator...\n","#########################################################################\n","Start Adversarial Training...\n","total_batch:  0\n","total_batch:  5\n","total_batch:  10\n","total_batch:  15\n","total_batch:  20\n","total_batch:  25\n","total_batch:  30\n","total_batch:  35\n","total_batch:  40\n","total_batch:  45\n","total_batch:  50\n","total_batch:  55\n","total_batch:  60\n","total_batch:  65\n","total_batch:  70\n","total_batch:  75\n","total_batch:  80\n","total_batch:  85\n","total_batch:  90\n","total_batch:  95\n","total_batch:  100\n","total_batch:  105\n","total_batch:  110\n","total_batch:  115\n","total_batch:  120\n","total_batch:  125\n","total_batch:  130\n","total_batch:  135\n","total_batch:  140\n","total_batch:  145\n","total_batch:  150\n","total_batch:  155\n","total_batch:  160\n","total_batch:  165\n","total_batch:  170\n","total_batch:  175\n","total_batch:  180\n","total_batch:  185\n","total_batch:  190\n","total_batch:  195\n","total_batch:  199\n"],"name":"stdout"}]}]}