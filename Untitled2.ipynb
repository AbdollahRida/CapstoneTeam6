{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVlNqBsKGbRmKW6EKWURC9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VzMvxXHoImjI","colab_type":"code","outputId":"d2b87091-4e5f-4c89-f30d-c42c1cdde6f5","executionInfo":{"status":"ok","timestamp":1582882696924,"user_tz":480,"elapsed":48162,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w4L8Ly71Iora","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive  -o nonempty"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMj09u6KIuL2","colab_type":"code","outputId":"2bdc1de1-1af7-42b3-882a-c0b11e00dc17","executionInfo":{"status":"ok","timestamp":1582882747402,"user_tz":480,"elapsed":18009,"user":{"displayName":"Charles He","photoUrl":"","userId":"13161158013461881879"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/SeqGAN-master\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5HzOgTAlIyYn","colab_type":"code","colab":{}},"source":["\n","os.chdir(path)\n","os.listdir(path)\n","\n","import numpy as np\n","import tensorflow as tf\n","import random\n","from dataloader import Gen_Data_loader, Dis_dataloader\n","from generator import Generator\n","from discriminator import Discriminator\n","from rollout import ROLLOUT\n","from target_lstm import TARGET_LSTM\n","import pickle\n","\n","#########################################################################################\n","#  Generator  Hyper-parameters\n","######################################################################################\n","EMB_DIM = 32 # embedding dimension\n","HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n","SEQ_LENGTH = 20 # sequence length\n","START_TOKEN = 0\n","PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n","SEED = 88\n","BATCH_SIZE = 64\n","\n","#########################################################################################\n","#  Discriminator  Hyper-parameters\n","#########################################################################################\n","dis_embedding_dim = 64\n","dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n","dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n","dis_dropout_keep_prob = 0.75\n","dis_l2_reg_lambda = 0.2\n","dis_batch_size = 64\n","\n","#########################################################################################\n","#  Basic Training Parameters\n","#########################################################################################\n","TOTAL_BATCH = 200\n","positive_file = 'save/data_preprocessing.txt'\n","negative_file = 'save/generator_sample.txt'\n","eval_file = 'save/eval_file.txt'\n","generated_num = 10000\n","\n","\n","def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n","    # Generate Samples\n","    generated_samples = []\n","    for _ in range(int(generated_num / batch_size)):\n","        generated_samples.extend(trainable_model.generate(sess))\n","\n","    with open(output_file, 'w') as fout:\n","        for poem in generated_samples:\n","            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n","            fout.write(buffer)\n","\n","\n","def target_loss(sess, target_lstm, data_loader):\n","    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n","    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n","    nll = []\n","    data_loader.reset_pointer()\n","\n","    for it in range(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n","        nll.append(g_loss)\n","\n","    return np.mean(nll)\n","\n","\n","def pre_train_epoch(sess, trainable_model, data_loader):\n","    # Pre-train the generator using MLE for one epoch\n","    supervised_g_losses = []\n","    data_loader.reset_pointer()\n","\n","    for it in range(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        _, g_loss = trainable_model.pretrain_step(sess, batch)\n","        supervised_g_losses.append(g_loss)\n","\n","    return np.mean(supervised_g_losses)\n","\n","\n","def main():\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    assert START_TOKEN == 0\n","\n","    gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n","    likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n","    vocab_size = 5000\n","    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n","\n","    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n","    target_params = pickle.load(open('save/target_params_py3.pkl','rb'))\n","    target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n","\n","    discriminator = Discriminator(sequence_length=20, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n","                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    sess.run(tf.global_variables_initializer())\n","\n","    # First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n","    #generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n","    gen_data_loader.create_batches(positive_file)\n","\n","    log = open('save/experiment-log.txt', 'w')\n","    #  pre-train generator\n","    print ('Start pre-training...')\n","    log.write('pre-training...\\n')\n","    for epoch in range(PRE_EPOCH_NUM):\n","        loss = pre_train_epoch(sess, generator, gen_data_loader)\n","        if epoch % 5 == 0:\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","            likelihood_data_loader.create_batches(eval_file)\n","            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n","            print ('pre-train epoch ', epoch, 'test_loss ', test_loss)\n","            buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n","            log.write(buffer)\n","\n","    print ('Start pre-training discriminator...')\n","    # Train 3 epoch on the generated data and do this for 50 times\n","    for _ in range(50):\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","        dis_data_loader.load_train_data(positive_file, negative_file)\n","        for _ in range(3):\n","            dis_data_loader.reset_pointer()\n","            for it in range(dis_data_loader.num_batch):\n","                x_batch, y_batch = dis_data_loader.next_batch()\n","                feed = {\n","                    discriminator.input_x: x_batch,\n","                    discriminator.input_y: y_batch,\n","                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                }\n","                _ = sess.run(discriminator.train_op, feed)\n","\n","    rollout = ROLLOUT(generator, 0.8)\n","\n","    print ('#########################################################################')\n","    print ('Start Adversarial Training...')\n","    log.write('adversarial training...\\n')\n","    for total_batch in range(TOTAL_BATCH):\n","        # Train the generator for one step\n","        for it in range(1):\n","            samples = generator.generate(sess)\n","            rewards = rollout.get_reward(sess, samples, 16, discriminator)\n","            feed = {generator.x: samples, generator.rewards: rewards}\n","            _ = sess.run(generator.g_updates, feed_dict=feed)\n","\n","        # Test\n","        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","            likelihood_data_loader.create_batches(eval_file)\n","            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n","            buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n","            print ('total_batch: ', total_batch, 'test_loss: ', test_loss)\n","            log.write(buffer)\n","\n","        # Update roll-out parameters\n","        rollout.update_params()\n","\n","        # Train the discriminator\n","        for _ in range(5):\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","            dis_data_loader.load_train_data(positive_file, negative_file)\n","\n","            for _ in range(3):\n","                dis_data_loader.reset_pointer()\n","                for it in range(dis_data_loader.num_batch):\n","                    x_batch, y_batch = dis_data_loader.next_batch()\n","                    feed = {\n","                        discriminator.input_x: x_batch,\n","                        discriminator.input_y: y_batch,\n","                        discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                    }\n","                    _ = sess.run(discriminator.train_op, feed)\n","\n","    log.close()\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[]}]}